% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{article}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{color} %vrivas, 15-Jan-2016
\usepackage{hyperref} %vrivas, 21-Jan-2016, for \url
\usepackage{graphicx} %vrivas, 27-Jan-2016, for images
\usepackage{listings}%vrivas, 27-Jan-2016, for code
\usepackage{algorithm} %jj, dec-8-2016, for code
\usepackage{algorithmic} %jj, dec-8-2016, for code
\usepackage{authblk} %jj, dec-20-2016, for authors

\begin{document}
%


\title{Time series forecasting using evolutionary neural nets implemented in a
  volunteer computing system}

% Author according to authblk
\author[1,3]{V.M. Rivas}
\author[1]{E. Parras-Guti\'{e}rrez}
\author[2,3]{JJ Merelo}
\author[2,3]{M.G. Arenas}
\author[4]{P. Garc\'{\i}a-Fern\'{a}ndez}

% Affiliations according to authblk
\affil[1]{Univ. of Jaen, Dept of Computer Sciences,\\
Campus Las Lagunillas s/n, 23071, Ja\'{e}n, SPAIN\\
\texttt{vrivas@ujaen.es},
\texttt{http://vrivas.es}}

\affil[2]{Depto. de Arquitectura y Tecnolog\'{\i}as de las Computadoras\\
Univ. de Granada, SPAIN
Univ. of Granada, Dept. of Computers, Architecture and Technology\\
C/ Periodista Daniel Saucedo s/n, 18071, Granada, SPAIN}

\affil[3]{GeNeura Team \\
  \url{http://geneura.wordpress.com}
}

\affil[4]{Depto. de Electr\'{o}nica y Tecnolog\'{\i}as de las
  Computadoras\\
 Univ. de Granada, SPAIN
 Univ. of Granada, Dept. of Electronics and Computer Technology}

\maketitle              % typeset the title of the contribution
\begin{abstract}
%##### NUEVA PROPUESTA DE ABSTRACT--- Maribel
{\em JsEvRBF} is a forecasting method that uses a genetic algorithm
for evolving radial basis function neural nets. The implementation
presented in this paper is able to run in most modern web browsers,
consequently everybody is allowed to contribute the algorithm
results participating in the experiments using their own browsers. The
use of browsers for experimentation is an interesting idea because the
scientists could use the huge number of inactive web browsers included
in each device as computation power, but it is a great challenge
too because the language support and performance varies as the
JavaScript Virtual Machine implementation changes hampering the same
algorithm is run in the same version or each browser.
The presented
results include a description of the experimentation process using
volunteers, which is one of the goals of this paper, and the
forecasting results using a currencies exchange dataset which are
comparable with previous results without the volunteer
participation. These results guarantee the viability of the proposal.

%#### cuando tengamos nuevos resultados puedo concretar un poco m ï¿½s la ï¿½ltima frase, que la he dejado un poco ambigï¿½a
% You can't say "the presented results do this". Say what you intented
% to prove and to what extent it has been proved.
% And please check below for the classical structure of an abstract:
% motivation, objective, methods, advance on results - JJ

%This paper presents the implementation of a time series
%forecasting algorithm, {\em jsEvRBF}, that uses genetic algorithm and neural nets
%in a way that can be run in must modern web browsers. Using browsers to run forecasting algorithms is a challenge,
%since language support and performance varies across implementations
%of the JavaScript virtual machine and vendor. However, their use will
%provide a boost in the number of platforms available for
%scientists. % A bit of motivation for the paper
%% Papers need to have: motivation, objective. And the first sentences
%% in the paragraph will have to go in that direction.
%{\em jsEvRBF} is
%written in JavaScript, so that it can
%be easily delivered to and executed by any device containing a
%web-browser just accessing an URL. The experiments show the results
%yielded by the algorithm over a  data set related to currencies
%exchange. % the results are what? Good? Better? Worse?
%Best results achieved  can be effectively compared against
%previous results in literature, though robustness of the new algorithm
%has to be improved. % What was your objective? Just running it?
%                    % Running it with a decent performance? - JJ

\end{abstract}
%
\textit{Keywords}: Time-series forecasting, evolutionary computation, radial basis function neural networks, Web-based programming, volunteer computation, fintech.

\section{Introduction}
% Web-based computing
Modern web browsers such as Firefox, Chrome, Edge, Safari and many
others differ from their predecessors in that they can do more than
render HTML code, show pictures and allow to navigate using links. In
fact, as far as browsers allow the executions of third-part programs
(applets) included in them
pages they downloaded, they have turned into wider
frameworks in which multi-platform applications can be executed.
Furthermore, new versions can natively decode and execute code that previously needed to be handled by a plug-in. An example of this is the Flash Player experience, since its usage has drastically reduced by HTML now that browsers are able to do a similar work \cite{Winokur11}.
% CITA En la que dice que Flash se dejï¿½ de usar en favor de HTML por el mismo Adobe                                Winokur, D. (2011). Flash to focus on PC browsing and mobile apps; Adobe to more aggressively contribute to HTML5. Retrieved, September 26, 2013 from http://blogs.adobe.com/conversations/2011/11/flash-focus.html
% Add to bibliography if needed - JJ- aï¿½adida, Maribel, y he aï¿½adido una frase en este primer pï¿½rrafo haciendo referencia a Flash
Actually, most of the programs interpreted and executed by browsers themselves are written in JavaScript, a programming language that was initially introduced \footnote{Press release announcing JavaScript, "Netscape and Sun announce JavaScript", PR Newswire, December 4, 1995. \url{http://bit.ly/JSPRESS}} to build interactive applications
with web pages more similar to what was available in desktop
applications \cite{Rauschmayer04}. % Don't know about this reference. Isn't it too recent? - JJ - I've added a footnote citing the press release where JavaScript was announced.
The web, as we currently use it, would not be the same without the
capability that JavaScript gives to the browser to make calculus,
to interact with the user, or to dynamically retrieve data from servers
without reloading the whole page, as it's done with AJAX\cite{powell2008ajax}. %Reference???.

Briefly, the history of JavaScript starts in the 1990s: the proprietary web browser Netscape Navigator had been created and was dominant, and in 1995, Brendan Eich was hired by Netscape company to design and implement a new language. At the same time, Netscape collaborated with Sun company to include in Navigator Java, its more static programming language, so it was questioned the needed of two programming languages: Java and a scripting language. Finally, they decided that the new scripting language had to make more accessible to non-Java programmers and web designers the support for Java applets \cite{Champeon08}. In May 1995, Eich designed a prototype in 10 days and was named first \emph{Mocha}, coined by the founder of Netscape Marc Andreessen, then \emph{LiveScript} and finally, in December 1995, \emph{JavaScript} \cite{Eich2010}, not because of the Java programming language, but to support Sun Microsystems.
JavaScript was standardized in 1997 by the European Computer Manufacturer's Association, or ECMA. According to the ECMA-262 standard, its real name is \emph{ECMAScript}, but everyone calls the language \emph{JavaScript} \cite{Flanagan06}.


%%Current everyday massively-used web pages and web applications (including those related to social networks) exist thanks to this language.
%% Its use has turned browsers into wider frameworks in which multi-platform applications can be run.



% Restricciones de JavaScript
JavaScript can be described as a general-purpose, object-based,
event-driven language. Although it was initially designed to be used only in the
browser, independent implementations running on the server were soon released; nowadays
it is used to write programs for both the client (the browser) and the
server thanks to frameworks like
\emph{node.js}~\cite{rauch2012smashing} 
% Why comment??- JJ - Because I wrote too much in the initial version and I had to reduce it according to the congress rules.
Adding JavaScript programs to web pages is simple: the code is inserted in the HTML code in plain text; then, it is downloaded by the browser that, finally,  interprets and executes it.
Thus, although web browsers allow the users to change the preferences
in order to disable the execution of any JavaScript code, this is
rarely done in these days. Even more, most standard web users are not
aware of the existence of this language, not realising their browsers
are working as programming environments in which code is analysed,
translated into a low-level language and executed. Consequently, and
for safety reasons, programs written in JavaScript are executed by
browsers using a sandbox model. This model imposes a series of
restriction that could be summarised as: the JavaScript program
inserted in a given web page cannot access any other resources than
the ones contained in that web page. This means that JavaScript programs are not allowed
to access the client's file system but only to store little pieces of text (called
cookies), and also when the user has to select a file to be sent
across the net.

The capabilities of JavaScript have led us to consider web browsers as agents that can download a web page containing a set
of data and the code of an evolutionary algorithm. This way, browsers can execute that algorithm, which is built to evolve neural nets. These neural nets can be applied to forecast an economic time-series. The whole process, from downloading the web page to obtain the forecasted values is done by the browser. Thus, using
this approach, any physical device in which a web browser can be installed (from
computers to smart TVs) can be potentially used to run our algorithm. 

% Genetic Algorithms and RBFNN
Both the problem being considered in this paper and the algorithm used
to solve it were introduced in \cite{rivas03:EvRBF}. On the one hand,
the problem consists on forecasting the values of the exchange rates
between two currencies for a four years period; data is weekly averaged. %This needs a citation. Where did you obtain data?
% Besides, could you use our own traffic data? - JJ 
% Yes of course... but I'd have to change the code to deal with trn+trn+tst files.
On the other hand, the algorithm (described in
section \ref{sec:algorithm}) is a reduced version of {\em EvRBF}
\cite{rivas03:EvRBF}, an evolutionary algorithm that makes Radial Basis
Function Neural Networks (RBFNN) to evolve.
% Now you're talking. You should say exactly what you have reduced,
% and make a more extensive description of EvRBF - JJ

RBFNN are well-known feed-forward neural nets with just one hidden and
one output layers. %\cite{Broomhead88}.
They have been successfully used to solve classification, function
approximation, and, as in this work, time-series forecasting problems
\cite{Broomhead88,Keogh03,Whitehead}.
The neurons in the output layer (only one neuron in this work) compute a weighted sum using outputs provided by hidden neurons, multiplied by some weights previously established, and adding a bias. The neurons in the hidden layer receive the input samples concerning the problem being resolved, and apply an activation function that  is a Radial Basis Function, i.e., a function returning a value that depends on the distance from the input values to a pre-established center of the function. The shape of this function is modified by means of a radius or width: the narrower this width, the higher the number of inputs that will not activate the neuron. Usually, Gaussian function is selected as the activation function, but many others can also be used.
Configuring an RBFNN in order to solve a task consists on: a) choosing
the activation function for hidden neurons, b) choosing the number of
hidden neurons, c) setting the parameters required by the activation
functions (i.e., center and radius of the RBF), and d) setting the
values for weights and bias. This last step can be easily computed
once the rest of components have been established using the Least Mean
Square method. {\em EvRBF} algorithm was designed to automatically
search for the best configuration of an RBFNN that solves the problem
being tackled, except for the activation function to be used that is
always a Gaussian function. 

% Time-series prediction

%Time-series can be considered as a special kind of function approximation where future values of the series are expressed as a function of past ones.
%There exist many methods in literature developed to forecast time-series references, being ARIMA \cite{BoxJenk} probably the most widely used.

In this paper the implementation of {\em EvRBF} for web browsers
(called {\em jsEvRBF}) has been compared to its original
implementation as well as to the methods used by Sheta in
\cite{Sheta2001}, the work in which the data set used in this paper
was introduced.

The rest of the paper is organized as follows: the state of the art in
distributed computing for financial prediction is presented in the
next section; our approach to this problem follows in Section
\ref{sec:algorithm}; the experimental setup to test our method and its
results are presented in Section \ref{sec:experiments}, and finally
and conclusions are drawn and future
lines of work presented in Section \ref{sec:conclusions}.

\section{State of the art}

Prediction of time series, specially financial ones, is considered a
hard problem requiring a great amount of computational resources,
since the model space that is explored is generally huge, so parallel
computing has been used at the same time than Genetic Programming,
\cite{santini2001genetic}, a technique that evolves rules and
expressions or neural networks of different types \cite{niska2004evolving,Mora2010,DBLP:conf/iwann/ArenasPRCJG09,RIIACastillo2008}, in which case
the parallel implementation is used to explore more efficiently the
space of different parallel architectures.

This is probably the reason why it has not been approached using
volunteer computing, which eschews fixed computing infraestructures,
relying on the computing resources lent by strangers
\cite{daniel:euromicro09,gecco07:workshop:dcor,DBLP:journals/corr/abs-0801-1210,DBLP:conf/gecco/MereloCGCRV16,baratloo1996charlotte,hwang2009determinants,web:BOINC}. Although
in these kind of networks a high sustained performance is difficult to
achieve \cite{DBLP:conf/lion/LaredoGFMACG11,DBLP:conf/gecco/MereloCGCRV16}, in the short term a
  good performance is achievable, with the only cost of setting up the
  experiment. That is why it has been applied successfully to protein
  structure prediction \cite{taufer2006predictor} and other
  experiments launched from the BOINC platform \cite{boinc_grid04}. In
  general, success has been met with the constraints of the difficulty
  in predicting the true performance of the meta-computer created by
  the experiment and its volunteers \cite{Merelo2016}. In fact, one of
  the major applications of non-volunteer computing in financial
  technology seems to be {\em illegal} bitcoin mining using the
  resources of infected computers \cite{plohmann2012case}.

We have already published the proof of concept and preliminary results
for this method in \cite{DBLP:conf/dcai/RivasPMAG16}. In that paper we
correct errors found and describe further experiments that measure the
performance obtained with different browsers and in different
conditions. In the same way, {\em jsEvRBF} has been evaluated with the same time-series
used by Sheta and de Jong \cite{Sheta2001} and the results were
published in \cite{DBLP:conf/dcai/RivasPMAG16}. For this paper, we
extend the state of the art including new experiments and new
results. %These kind of claims 
                                %should go to the state of the art -
                                %JJ, Ok, lo cambio, Maribel 


\section{The {\em jsEvRBF} implementation} %Is it an algorithm or an
                                %implementation? - JJ, Yo creo que es
                                %una implementaciï¿½n, el algoritmo no
                                %es nuevo.
%Eso es lo que yo digo. Pero en este caso es un método basado en un
%algoritmo, habría que decirlo así, no simplemente "implementation",
%porqeu es algo más - JJ
\label{sec:algorithm}
% The {\em EvRBF} general skeleton
{\em jsEvRBF} denotes the implementation in JavaScript of our previously existing evolutionary algorithm {\em EvRBF} \cite{rivas03:EvRBF} ; writing this algorithm in JavaScript allows it execution in web browsers.
The main feature of this algorithm is that individuals are complete RBFNN, and, for this reason,
the operators have been specifically designed to cross and mutate them. The only difference of the new implementation with respect to the original algorithm is the number of operators that have been implemented: while {\em EvRBF} had 6 different operators, {\em jsEvRBF} has only 3, those that can be executed faster in the browser.%The
                                %same special algorithms or others? - JJ- No entiendo  lo que quieres decir J, Maribel
Apart from using RBFNN as individuals, {\em jsEvRBF} is a standard generational algorithm. The number of individuals in every generation remains fixed; it uses tournament selection to choose the individuals to reproduce , and elitist replacement  makes best individuals to form part of every new generation\footnote{The code can be downloaded or forked from
  \url{http://bit.ly/jsEvRBF}; its use is restricted under the terms
  of the Apache 2.0 license.}.

The implementation of both the library for RBFNN ( {\em
  jsRBFNN}\footnote{\url{http://bit.ly/jsRBFNN}}) and the library for the evolutionary algorithm  {\em
  jsEO}\footnote{\url{http://bit.ly/js-EO}}, have been also written by
our research group. Thus, {\em jsRBFNN} implements the neural nets and also the LMS
training algorithm. %This should be explained somewhere. This is a
                    %financial journal - JJ
Regarding  {\em jsEO} \cite{EvoStar2014:jsEO}, it is a more
complex framework that allows the generation of many kinds of
evolutionary algorithms, making easier the task of creating new types
of individuals and/or operators. Figure \ref{fig:class_diagram}
graphically shows the dependencies between {\em jsEvRBF} and these
libraries.


As a standard evolutionary algorithm, the skeleton of {\em jsEvRBF} is
as follows:

\begin{algorithm}
\caption{jsEvRBF program}
\begin{algorithmic}
\STATE Create, train and evaluate an initial population of $p$ individuals.
\FOR{generation = 1 \TO n}
   \STATE{Select a subpopulation of $q$ individuals}
   \STATE{Create $q$ new individuals applying an operator to each one in subpopulation}
   \STATE{Train and evaluate the $q$ new individuals}
   \STATE{Join and sort both old and new populations}
   \STATE{Remove the $q$ worst individuals}
\ENDFOR
\STATE Send the forecasting done by the best individual in last generation to the server
\end{algorithmic}
\end{algorithm}

\begin{figure}[!ht]
\includegraphics[width=120mm]{class-diagram.jpg}
\caption{Class diagram of the jsEvRBF algorithm, showing the way it depends on the jsEO general framework and the jsRBFNN library.}
\label{fig:class_diagram} % Do we need this thing? ~JJ -  I really like it. BTW, I wish we had realised in EO that eo wasn't a class, but an Interface  ~vrivas
\end{figure}


%The individuals are RBFNN, any of them composed of a set of hidden neurons implemented as a vector of objects. Every hidden neuron stores its center and radius, and the global RBFNN stores the weights and bias.
In order to compare individuals, so that best ones can be chosen to reproduce and worst ones disappear in every new generation, the fitness of every individual is computed as the inverse of the RMSE. Using the inverse allows to magnify the differences between individuals, so that the greater the fitness, the better the individual.

The operators used in {\em jsEvRBF's} are the following:

\begin{itemize}
\item {\em Crossover (namely XOver).} Takes two individuals as inputs and operates by randomly selecting a set of neurons from first individual and another set (probably of a different size) from the second one. After this, those sets are interchanged.
\item {\em Center mutator (namely CenterMut.} Modifies a given percentage of the centers of one individual by setting them to random values in the range defined by the input dimension.
\item {\em Radius mutator (namelu RadiusMut.)} Similarly to \emph{CenterMut}, this operator modifies a percentage of the radius of the neurons of one individual, choosing a new value in the same way explained before.
\end{itemize}


Finally, the following set of parameters has to be set in
order to run the {\em jsEvRBF} algorithm: % What's the point of this?
                                % - JJ
                                % In my opininion, if someone wants to reproduce our algorithm, they need to know the parameters we are using.
\begin{itemize}
\item{\em trnSamples}: Set of samples to train the nets; some of them, randomly chosen, it will be used as centers for the RBF of the individuals composing the initial population.
\item{\em valSamples}: Set of samples to compute fitness of every new created individual.
\item{\em inputDimension}: Dimension of inputs.
\item{\em numNeurons}: Number of neurons for individuals of the first population.
\item{\em popSize}: Number of individuals per population.
\item{\em numGenerations}: Number of generations for the evolutionary algorithm.
\item{\em tournamentSize}: Number of individuals to consider when selecting one of them to reproduce.
\item{\em replaceRate}: Rate of individuals to be replaced in every new generation.
\item{\em xOverRate}: Determines the number of individuals to which xOver operators will be applied.
\item{\em mutRate}: Determines the number of individuals to which mutator operators will be applied.
\item{\em mutPower}: Determines the number of neurons that will be changed by mutator operators, when applied to an individual.
\end{itemize}

The exact value used in each of these parameters for the experiments carried out is described in the next section.
% Specific characteristics of this implementation
% Operators
\section{Experiments and results}
\label{sec:experiments}
The data used in our experiments is composed of 208
weekly averaged observations representing the exchange rates between
Bristish pound and US dollar from 31 December 1979 to 26 December
1983\footnote{The source of the information, thanks to the work done
  by Prof. Werner Antweiler from the University of British Columbia,
  Vancouver, Canada, is available from
  \url{http://pacific.commerce.ubc.ca/xr/data.html}.}. This data set is exactly the same and
has been used in the same way than Sheta and de Jong in
\cite{Sheta2001}; thus, only one randomly chosen half of the
dataset has been used to train and validate the obtained neural nets,
while the whole data set has been used to compute the generalisation
power of the best neural net found in every execution of the
algorithm.

In order to test the capabilities of web browsers in financial time-series forecasting,
two experiments have been carried out using {\em jsEvRBF},
additionally to those performed in our previous work
\cite{DBLP:conf/dcai/RivasPMAG16}.
% This has to be revised, since experiments are different now. - JJ
For the first experiment (see section \ref{sec:first-experiment}) and in order to test the performance in a volunteer computing set up, we
have effectively opened the system to a wide community of users, so that many different
devices, used from many operating systems and web browsers have
participated in it. Besides, the second experiment (section \ref{sec:second-experiment}) is intended to give an idea of the
baseline performance of every browser. For this reason, we have
also tested the algorithm in isolated computers, using a single web
browser, so that we can focus in the forecasting process itself and
also assess the differences among different browser; this will also
help, in the case of a controlled environment such as an office or
laboratory, to choose the browser with the best performance
available. 
In any case, participating in the experiment only required to connect
to the URL: \url{http://150.214.178.89}; no special technical knowledge was necessary to run the
algorithm since it was automatically executed once the web page was
loaded. Figure \ref{fig:example-of-execution} shows the page users
could read when accessing the specified URL.  
Every time an user accessed that URL, the web page containing the algorithm was loaded in their computer and $10$ complete, independent executions of the algorithm were done. Once finished every execution, the results of forecasting the whole data set yielded in that execution was sent to the sever. After then $10$ executions have finished, the page was reloaded again if the user did not close the window in less than $10$ seconds. 


Public information (browser and operating system versions) about the clients executing the algorithm and the results they yielded were received
by a server programmed also in JavaScript (using {\em node.js}) and
stored in a No-SQL database managed by {\em Mongo}; the language to
query the database was JavaScript too. The proof of concept results
have been published in \cite{DBLP:conf/dcai/2016de}; they showed some
promise of the method, as well as significant differences among
browsers. In this paper we report a new set of experiments and update
single-browser performance with updated browser versions.


\begin{figure}[!ht]
\includegraphics[width=120mm]{example-of-execution.png}
\caption{The look of the web page in which the {\em jsEvRBF} algorithm was loaded and executed.}
\label{fig:example-of-execution}
\end{figure}
% Maybe add new version of this screen - JJ
% Yes, of course. The web pages has changed.

The values used to run the algorithm are shown in table \ref{tab:parameters-experiments}. These values are exactly the same than those used in the first version of the algorithm introduced in \cite{rivas03:EvRBF}

\setlength{\tabcolsep}{10pt}
\begin{table}
\caption{Configuration of parameters for the experiments. The number of samples ({\em trnSamples} and {\em valSamples}) is still approximated since they were randomly chosen in every execution.}
\label{tab:parameters-experiments}
\begin{center}
\begin{tabular}{rr|rr}
{\bf Parameter} & {\bf Value} &
{\bf Parameter} & {\bf Value}\\
\hline
{\em trnSamples} & $\approx 90$ &
{\em valSamples} & $\approx 15$  \\
{\em inputDimension} &   1 &
{\em numNeurons} &  10 \\
{\em popSize} &  15 &
{\em numGenerations} & 10  \\
{\em tournamentSize} &  3 &
{\em replaceRate} &   0.2 \\
{\em xOverRate} &  0.2&
{\em mutRate} &   0.8  \\
{\em mutPower} &  0.5 &
{\em Executions per page load} & 1 \\
\hline
\end{tabular}
\end{center}
\end{table}

% Second one: only one browser, complete executions
\subsection{Testing implementation's validity and user's willingness}
\label{sec:first-experiment}
The first experiment was designed to check the willingness of web
users when asked to participate in a volunteer computation
experiment. In this sense, a call for participation in the experiment was
made using social networks, mainly Facebook and Twitter. The experiment was
open for two weeks (from 29/Dec/2016 to 13/Jan/2017), and users only
had to open in their browsers the previously cited URL.  
Along the 15 days that lasted the experiment, $41853$ executions done
by $55$ users were completed; thus, in average, every user ran the
experiment close to 
$761$ times. % Next should go in a table - JJ 
The most widely used operating system was Android
( $54.5\%$) of users, followed by Windows and Mac OS ($18.2\%$ any of
them), and Linux ($9.1\%$). %Next to another table - JJ
Regarding the browsers, Chrome was the
preferred by users ($69.1\%$), followed by  Firefox in $20.0\%$, and
finally Safari, that was used in $10.9\%$ of the cases. These first
results clearly show the validity of the implementation regarding its
multi-platform availability and easiness of use, but also the
difficulties that arise when trying to involve users in this kind of
volunteer computation, since only $55$ users agreed to execute our
algorithm. It also shows the kind of users that can be expected:
users from mobile devices, most of them using the default browser,
Chrome. 

With respect to the accuracy of the forecasting performed in each
experiment, the MSE of the best 
solution found along all the executions was $6\times10^{-4}$. As can
be see in table \ref{tab:comparison-first-experiment}, this solution
is as good as the average one found by the original algorithm (EvRBF)
and much better that our previously published results \cite{DBLP:conf/dcai/RivasPMAG16}. The later is due to the fact that every executions has
finished with valid values, since new versions of Safari and Chrome
browsers (i.e., AppleWebKit-based browsers) are not affected by the
\em{NaN} error we found in our previous work. Taking into account the
$41853$ executions, the MSE turns to be $2 \times
10^{-3}\pm8\times10^{-1}$, which is comparable to the results yielded
by the MSE-LSE algorithm introduced by Sheta in \cite{Sheta2001}. 

%Finally, table \ref{tab:all-error-values-experiment-1} shows the rest of error-measure values yielded by {\em jsEvRBF}, in order to easier future works. 


\setlength{\tabcolsep}{10pt}
\begin{table}
\caption{First experiment: comparison of MSE yielded by the {\em
    jsEvRBF} algorithm and the ones cited in \cite{Rivas04}. Row {\em
    jsEvRBF'16} includes the results presented in PAAMS congress,
  while row {\em jsEvRBF'17} belongs to this new set of
  experiments. Errors have computed over the full set of data.} 
\label{tab:comparison-first-experiment}
\begin{center}
\begin{tabular}{lcc}
{\bf Method} & {\bf Average MSE} & {\bf Best MSE} \\
\hline
\bf EvRBF & $\mathbf{6 \times 10^{-4} \pm 2 \times 10^{-4}}$ &$\mathbf{4 \times 10^{-4}}$ \\
MSE-GA & $9 \times 10^{-4}$ & N/A \\
MSE-LSE &  $1 \times 10^{-3}$ & N/A \\
{\em jsEvRBF'17} & $\mathit{2 \times 10^{-3} \pm 8 \times 10^{0}}$ & $\mathit{6 \times 10^{-4}}$  \\
jsEvRBF'16 & $2 \times 10^{-2} \pm 5 \times 10^{0}$ & $6 \times10^{-4}$  \\
\hline
\end{tabular}
\end{center}
\end{table}

% Second one: only one browser, complete executions
\subsection{Testing single-browser performance}
\label{sec:second-experiment}

This second experiment is intended to detect the different behaviour of {\em jsEvRBF} depending of the browser in which it is executed. Although the JavaScript language is the same for any browser, the implementation of the
JavaScript virtual machine they use introduces some differences that may affect to the complete process. In order to control the results yielded by each browser, the web page was loaded only once in any of them; thus, the results presented in this paper are averaged over $10$ executions. 

Table \ref{tab:results-per-browser-second-experiment} shows the results yielded by the different browsers.

\setlength{\tabcolsep}{10pt}
\begin{table}
\caption{Single-browser experiment: comparison of MSE yielded by the {\em
    jsEvRBF} algorithm and the ones cited in
  \cite{rivas03:EvRBF}. Errors have computed over the full set of
  data, having executing the algorithm 10 times.} % 
\label{tab:comparison-second-experiment}
\begin{center}
\begin{tabular}{lcc}
{\bf Method} & {\bf Average MSE} & {\bf Best MSE} \\
\hline
EvRBF & $6 \times 10^{-4} \pm 2 \times 10^{-4}$ &$4 \times 10^{-4} \pm 2 \times 10^{-4}$ \\
{\em jsEvRBF} & $8 \times 10^{-4} \pm 2 \times 10^{-7}$ & $6\times10^{-4}$  \\
MSE-GA & $9 \times 10^{-4}$ & N/A \\
MSE-LSE &  $12 \times 10^{-4}$ & N/A \\
\hline
\end{tabular}
\end{center}
% Mark the best and draw some conclusion - JJ
\end{table}

%Finally, and as we did before, table \ref{tab:all-error-values-experiment-2} summarizes the values computed for this experiment with regard to the complete set of error measures.
%
%\setlength{\tabcolsep}{3pt}
%\begin{table}
%\caption{Second experiment: complete set of error measures yielded {\em jsEvRBF} computed over the entire set of data.}
%\label{tab:all-error-values-experiment-2}
%\begin{center}
%\begin{tabular}{lc|lc}
%{\bf Error Measure} & {\bf Value} & {\bf Error measure} & {\bf Value} \\
%\hline
%
%MSE & $7.5601e-4 \pm 1.6728e-7$  &
%RMSE & $2.7398e-2 \pm 5.3461e-5$ \\
%MAE & $2.1099e-2 \pm 3.4528e-5$  &
%MdAE & $1.6385e-2 \pm 4.0988e-5$ \\
%MAPE & $1.1091e+0 \pm 1.0119e-1$  &
%MdAPE & $8.7471e-1 \pm 1.2897e-1$ \\
%RMSPE & $1.4293e+0 \pm 1.5105e-1$  &
%RMdSPE & $8.7471e-1 \pm 1.2897e-1$ \\
%sMAPE & $1.1065e+0 \pm 9.8722e-2$  &
%sMdAPE & $8.7381e-1 \pm 1.2609e-1$ \\
%MASE & $1.1515e+0 \pm 1.0601e-1$  &
%RMSSE & $1.4940e+0 \pm 1.6138e-1$ \\
%MdASE & $8.9537e-1 \pm 1.2759e-1$  \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}


\section{Conclusions}
\label{sec:conclusions}
Web-browsers have been proved to be a good alternative to achieve
distributing computation, minimizing the effort needed to ensure
cross-platform compatibility and easiness of use. To show it, we have
introduced the {\em jsEvRBF} algorithm;
written in JavaScript, this algorithm can be executed in web-browsers
when users access to a specified URL. The web page downloaded by the
browser contains the code of this evolutionary algorithm, able to
build, train and evolve RBFNN used to perform time-series forecasting
over a set of data related to currency exchange. % which is
                                % interesting because... -JJ

The experiments carried out show that the approach is valid,
although some research must still be done in order to determine the
reasons that make AppleWebKit-based browsers (mainly Chrome and
Safari) not work properly.

Future work includes the use of the
algorithm in an {\em island model}  framework in which good
individuals be distributed to the clients running the algorithm.
%since {\em jsEO}, the underlying
%library supporting {\em jsEvRBF}, has been implemented with this
%feature.



\section*{Acknowledgements}
 This work has been supported in part by:
 Ministerio de Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad under (preselected as granted) project TIN2014-56494-C4-3-P (UGR-EPHEMECH)
 , CEI2015-MP-V17 of the Microprojects program 2015 from CEI BioTIC Granada
 , PROY-PP2015-06 (Plan Propio 2015 UGR),
 and
 PRY142/14 (Este proyecto con num. de referencia: PRY142/14 ha sido financiado ï¿½ntegramente por la Fundaci\'{o}n P\'{u}blica Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigaci\'{o}n)\footnote{The description in Spanish is mandatory.}.


%
% ---- Bibliography ----
%

\bibliographystyle{apalike}
\bibliography{dcai,geneura,volunteer}

\end{document}
